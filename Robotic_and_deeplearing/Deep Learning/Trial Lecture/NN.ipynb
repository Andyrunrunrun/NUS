{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Since our aim is to build machine learning models (whether statistical or neural network) to understand IoT data, let's begin by building some simple models in Python. \n",
    "\n",
    "In this lecture we look at implementing neural networks in Python. For the most part we will be using Keras, a high-level model-based library for implementing highly complex and sophisticated deep networks. \n",
    "\n",
    "But we will start small, and actually code a learning law on our own. We start by implementing a simple Single Layer Perceptron to learn the Lorry/Van Classification Problem that's in the lecture. The table below shows the data we have. We use -1 to mean \"Lorry\" and \"1\" to mean \"Van\":\n",
    "\n",
    "|Mass    |Length     |Class   |\n",
    "|:------:|:---------:|:------:|\n",
    "|10      |6          |-1      |\n",
    "|20      |5          |-1      |\n",
    "|5       |4          |1       |\n",
    "|2       |5          |1       |\n",
    "|3       |6          |-1      |\n",
    "|10      |7          |-1      |\n",
    "|5       |9          |-1      |\n",
    "|2       |5          |1       |\n",
    "|2.5     |5          |1       |\n",
    "|20      |5          |-1      |\n",
    "\n",
    "Let's begin by importing Numpy and creating our table by defining a function to create our datasets. Each input example contains the mass and length of the vehicle, and the labels are -1 for truck and 1 for van. The make_dataset function returns a 10x2 matrix for the input, and a 10x1 vector for the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create our dataset\n",
    "def make_dataset():\n",
    "    train_data = np.array([[[10, 6]], [[20, 5]], [[5, 4]], \n",
    "                           [[2, 5]], [[3, 6]], [[10, 7]], \n",
    "                           [[5, 9]], [[2, 5]], [[2.5, 5]], \n",
    "                           [[20, 5]]])\n",
    "    train_labels = np.array([-1, -1, 1, 1, -1, -1, -1, 1, 1, -1])\n",
    "    \n",
    "    return (train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initialize the SLP. We define the SLP as a dictionary defined as follows:\n",
    "\n",
    "slp = {\n",
    "\n",
    "\"inputs\":<1x3 input vector>,\n",
    "\n",
    "\"weights\":<3x1 weights>,\n",
    "\n",
    "\"output\":<1x1 output>\n",
    "\n",
    "}\n",
    "\n",
    "Although we have only 2 inputs, our input is defined as 1x3 as we need to include the bias. There is only one output, and thus the weights will be 3x1 matrix of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SLP:\n",
    "# We store our SLP as a dictionary. There are 3 inputs since we have Mass,\n",
    "# Length, and a bias which is always 1.0. There are 3 weights to connect\n",
    "# the 3 inputs to the output, and a single output\n",
    "\n",
    "def init_slp(slp):\n",
    "    slp['inputs'] = np.array([0.0, 0.0, 1.0])\n",
    "    slp['weights'] = np.random.randn(3, 1)\n",
    "    slp['output'] = np.array(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the meat of the SLP: The feedforward and learn functions. The feedforward function is defined as:\n",
    "\n",
    "$$\n",
    "f(in, w) = \\tanh\\left(\\sum_{i=0}^{n-1}in_i \\times w_{i,0}\\right)\n",
    "$$\n",
    "\n",
    "Since we have defined our input as a $1\\times3$ matrix and the weights as a $3\\times1$ matrix, the feedforward is simply a matrix multiply.  We use a tanh transfer function since this maps us between -1 and 1. We set a parameter *alpha* to control the speed of learning. The learning function returns the absolute error, which we will later use to compute the mean absolute error (MAE) across all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feedfoward\n",
    "def feed_forward(slp, inputs):\n",
    "    # Take dot-product of the inputs and the weights\n",
    "    slp[\"inputs\"][0:2] = inputs\n",
    "    slp[\"output\"] = np.tanh(np.matmul(slp[\"inputs\"], slp[\"weights\"]))\n",
    "    return slp[\"output\"]\n",
    "\n",
    "def learn(slp, alpha, inputs, target):\n",
    "    feed_forward(slp, inputs)\n",
    "    \n",
    "    # Find error\n",
    "    E = target - slp['output']\n",
    "    slp[\"weights\"] = np.add(slp[\"weights\"], (alpha * E[0] \n",
    "                                          * slp['inputs'].reshape(3,1)))\n",
    "    return abs(E[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our SLP and train it. We iterate 600 times and print the MAE every 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Average Absolute Error: 1.36\n",
      "Iteration 50, Average Absolute Error: 0.45\n",
      "Iteration 100, Average Absolute Error: 0.44\n",
      "Iteration 150, Average Absolute Error: 0.44\n",
      "Iteration 200, Average Absolute Error: 0.43\n",
      "Iteration 250, Average Absolute Error: 0.43\n",
      "Iteration 300, Average Absolute Error: 0.42\n",
      "Iteration 350, Average Absolute Error: 0.42\n",
      "Iteration 400, Average Absolute Error: 0.41\n",
      "Iteration 450, Average Absolute Error: 0.39\n",
      "Iteration 500, Average Absolute Error: 0.01\n",
      "Iteration 550, Average Absolute Error: 0.01\n",
      "Iteration 600, Average Absolute Error: 0.01\n"
     ]
    }
   ],
   "source": [
    "slp = {}\n",
    "init_slp(slp)\n",
    "feed_forward(slp, np.array([[20.0, 5.0]]))\n",
    "\n",
    "(train_in, train_out) = make_dataset()\n",
    "for i in range(601):\n",
    "    ctr = 0\n",
    "    E = 0\n",
    "    for j, data in enumerate(train_in):\n",
    "        ctr = ctr + 1\n",
    "        E = E + learn(slp, 0.1, data, train_out[j])\n",
    "    \n",
    "    if (i % 50) == 0:\n",
    "        print(\"Iteration %d, Average Absolute Error: %3.2f\" % (i, E / ctr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the MAE settles at a decent value of 0.01. Now we can try three sample inputs:\n",
    "\n",
    "|Mass    |Length     |\n",
    "|:------:|:---------:|\n",
    "|12      |7          |\n",
    "|3       |5          |\n",
    "|15      |12         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass\tLength\tClass\n",
      "-----\t------\t-----\n",
      "12.0\t7.0\ttruck\n",
      "3.0\t5.0\tvan\n",
      "15.0\t12.0\ttruck\n"
     ]
    }
   ],
   "source": [
    "test_inputs = np.matrix([[12, 7], [3, 5], [15, 12]])\n",
    "\n",
    "print(\"Mass\\tLength\\tClass\")\n",
    "print(\"-----\\t------\\t-----\")\n",
    "\n",
    "for x in test_inputs:\n",
    "    y = feed_forward(slp = slp, inputs = x)\n",
    "    veh_type = 'truck' if y<=0.0 else 'van'\n",
    "    print(\"%3.1f\\t%3.1f\\t%s\"% (x[0,0], x[0,1], veh_type))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't put aside some of the training data for testing (there's only 10 of them), we don't have a \"gold standard\" to evaluate how good this SLP is. That's alright, since our main aim was to see how to implement the learning law. In any case the outputs here seem consistent with the training data (large mass, length -> truck, otherwise it's a van.)\n",
    "\n",
    "## Keras Models\n",
    "\n",
    "In this course we will use the Keras library to implement our neural networks. Keras is a convenient high-level library that is built on top of Google's TensorFlow project, which is in turn a very large and complex library for handling vector arithmetic.\n",
    "\n",
    "TensorFlow can be configured to run on graphics processing units (GPUs) or Tensor Processing Units (TPUs), but you will need to install GPU or TPU versions separately from what is already provided in this course.\n",
    "\n",
    "Full documentation on Keras can be found at [Keras docs](https://keras.io/api/)\n",
    "\n",
    "Keras is much easier to learn than TensorFlow, and provides flexibility in using other deep learning backends like Microsoft CNTK and Theano. It is however less flexible and powerful.  The latest versions of TensorFlow include Keras, but here we will use Keras on its own (built on TensorFlow) rather than as a library within TensorFlow.\n",
    "\n",
    "Keras is also definitely much more convenient to use than NumPy. ;)\n",
    "\n",
    "While TensorFlow is centred around the idea of transformations to tensors (basically vectors and matrices) as it flows through the system (hence the name TensorFlow), Keras is centred around higher level models. In Keras there are two models:\n",
    "\n",
    "    1. Sequential Model\n",
    "    \n",
    "    In the Sequential Model neural network layers are just simply stacked together. Keras infers how to connect the layers together based on tensor sizes between the outputs of one layer and the inputs of the next layer. The Sequential Model is easy to understand but restricts you to a simple stacking model.\n",
    "    \n",
    "  ![Stacked neural network layers](./Images/fully-connected.png)\n",
    "    \n",
    "    2. The Functional API\n",
    "    \n",
    "    The Function API is a bit more complicated in that it requires you to specify how to connect one layer to another. However it allows you to build much more sophisticated architectures, like neural networks that take inputs further inside (as opposed to only the input layer), or incorporate outputs from various other neural networks to produce a single consolidated output.\n",
    "    \n",
    "![More complex network](./Images/complex.png)\n",
    "    \n",
    "In this document we will look at how to create the simpler Sequential model, and see how to use the Functional API in a future session. Let's begin with building a simple Multi-Layer Perceptron using the Sequential Model to recognize handwritten digits from the famouse MNIST dataset.\n",
    "\n",
    "The MNIST dataset consists of a 28x28 black and white images of handwritten digits:\n",
    "\n",
    "![MNIST set](./Images/mnist.jpg)\n",
    "\n",
    "Our job then is to build a classifier that takes a 28x28 image and classify it as one of the 10 digits.\n",
    "\n",
    "## Imports\n",
    "\n",
    "We begin by importing:\n",
    "\n",
    "    - Dense: The Keras implementation for a layer of normal fully-connected NN nodes\n",
    "    - Dropout: A special layer to control overfitting. More on this in later lectures.\n",
    "    - Sequential: The Keras Sequential Model.\n",
    "    - SGD: Stochastic Gradient Descent, a modified version of the gradient descent method that you saw in the lecture.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import Dense and Dropout\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Bring in the MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# This is a utility function that generates \"one-hot\" vectors. E.g. if \n",
    "# there are four categories, and the samples have the following output\n",
    "# labels: (0, 3, 1, 2), this will be converted to the following labels:\n",
    "# 0: [1, 0, 0, 0]\n",
    "# 3: [0, 0, 0, 1]\n",
    "# 1: [0, 1, 0, 0]\n",
    "# 2: [0, 0, 1, 0]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Finally we bring in the optimizer\n",
    "from tensorflow.keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing and Building the Model\n",
    "\n",
    "Now let's begin building our model. The weights of the neural networks are called \"parameters\", and these are decided upon using an optimization algorithm. However we ourselves need to decide on \"hyperparameters\", which refer to the design of the NN:\n",
    "\n",
    "    - The size and shape of the input\n",
    "    - Encoding for the input\n",
    "    - # of hidden layers\n",
    "    - Size of each hidden layer\n",
    "    - Transfer functions\n",
    "    - Size of the output\n",
    "    \n",
    "Some of these are easy to determine. Since our inputs are 28x28 images and it's easier to work with a single dimension vector, we will reshape them into a single 784 element input. Hence the input layer will consist of 784 input nodes. We will scale all inputs to between 0 and 1 for performance reasons. There are 10 digits and thus 10 output nodes.\n",
    "\n",
    "For the rest we apply a combination of two well respected design techniques called \"intuition\" and \"guesswork\" and produce the following design:\n",
    "\n",
    "    - # of input nodes: 784\n",
    "    - Encoding: Scale between 0 and 1\n",
    "    - # of hidden layers: 2\n",
    "    - Sizee of hidden layer 1: 1024 nodes\n",
    "    - Transfer function: ReLU (see below)\n",
    "    - Size of hidden layer 2: 256 nodes\n",
    "    - Transfer function: ReLU\n",
    "    - Size of output: 10 nodes\n",
    "    - Transfer function: Softmax\n",
    "    \n",
    "The ReLU, Sigmoid (similar to Softmax) and other transfer functions are shown below. We saw these in the lecture:\n",
    "\n",
    "![Transfer Functions](./Images/transfer.png)\n",
    "\n",
    "We also add a \"dropout\" layer which randomly drops a percentage of the nodes for training, to reduce overfitting. We will look at this in more detail in a later lecture.\n",
    "\n",
    "Let's build our network. We first create a Sequential model then add the layers. We then call create the optimizer and call compile to finalize the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(1024, input_shape = (784, ), activation = 'relu'))\n",
    "\n",
    "# Randomly drop 30% of this layer for training\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer. No need to specify the input shape since\n",
    "# Keras can infer it from the previous layer (1024 nodes)\n",
    "# As before we randomly drop 30% of the nodes for training.\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Finally our output with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Create a Stochastic Gradient Descent optimizer with a learn rate of 0.01\n",
    "#  Finally there's a momentum of 0.9 which helps control\n",
    "# \"overshoot\"\n",
    "sgd  = SGD(learning_rate = 0.01, momentum = 0.9)\n",
    "\n",
    "# Now compile the model. We use a \"categorical cross entropy\" loss function\n",
    "# which is more sophisticated than the simple mean-squared loss\n",
    "# function in the lecture and well suited for classification problems.\n",
    "# We will look at it again at a later lecture.\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd,\n",
    "             metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "This is literally it! We've built the network! Now let's bring in the MNIST dataset. We will reshape the data from 28x28x1 to 784x1, then convert it to 32-bit floats and divide by 255 to scale the pixel values to between 0 and 1. We will also convert the labels to one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: train_y:  [5 0 4 ... 5 6 8]\n",
      "After: train_y:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data and reshape it. train_x.shape[0] contains the number\n",
    "# of rows.\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "train_x = train_x.reshape(train_x.shape[0], 784)\n",
    "test_x = test_x.reshape(test_x.shape[0], 784)\n",
    "\n",
    "# Conver to 32-bit floats\n",
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')\n",
    "\n",
    "# Now scale from 0 to 255 to 0 to 1:\n",
    "train_x = train_x / 255.0\n",
    "test_x = test_x / 255.0\n",
    "\n",
    "# Now convert the labels to one-hot vectors\n",
    "print(\"Before: train_y: \", train_y)\n",
    "train_y = to_categorical(train_y, 10)\n",
    "print(\"After: train_y: \", train_y)\n",
    "test_y = to_categorical(test_y, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Now that we have built the network, and loaded and properly encoded the data, let's start training. Here we will call the \"fit\" method, and shuffle the data around. We will train for 10 epochs in batches of 60 samples. \"Batches\" are useful for controlling memory usage, especially when you are working in memory limited environments like GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 2ms/step - loss: 0.3653 - accuracy: 0.8901 - val_loss: 0.1464 - val_accuracy: 0.9562\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1585 - accuracy: 0.9519 - val_loss: 0.0982 - val_accuracy: 0.9697\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.9659 - val_loss: 0.0865 - val_accuracy: 0.9722\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0902 - accuracy: 0.9723 - val_loss: 0.0767 - val_accuracy: 0.9760\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0743 - accuracy: 0.9772 - val_loss: 0.0677 - val_accuracy: 0.9782\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0623 - accuracy: 0.9807 - val_loss: 0.0640 - val_accuracy: 0.9800\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0555 - accuracy: 0.9834 - val_loss: 0.0612 - val_accuracy: 0.9808\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0489 - accuracy: 0.9845 - val_loss: 0.0579 - val_accuracy: 0.9817\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0432 - accuracy: 0.9861 - val_loss: 0.0582 - val_accuracy: 0.9821\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0381 - accuracy: 0.9883 - val_loss: 0.0539 - val_accuracy: 0.9825\n",
      "Done testing. Now evaluating:\n"
     ]
    }
   ],
   "source": [
    "model.fit(x = train_x, y = train_y, shuffle = True, batch_size = 60, \n",
    "          epochs = 10, validation_data = (test_x, test_y))\n",
    "\n",
    "print(\"Done testing. Now evaluating:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally once training is over we evaluate the network for performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0593 - accuracy: 0.9819\n",
      "Final loss is 0.06, accuracy is 0.98.\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x = test_x, y = test_y)\n",
    "print(\"Final loss is %3.2f, accuracy is %3.2f.\" % (loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
