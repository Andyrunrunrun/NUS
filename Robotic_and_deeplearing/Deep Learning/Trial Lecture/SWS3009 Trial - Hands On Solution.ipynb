{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWS3009 Trial Lecture - Neural Networks\n",
    "\n",
    "Please fill in your student number and name below. Each student is to make one submission.\n",
    "\n",
    "| Student Number | Name                 |\n",
    "|:--------------:|:--------------------:|\n",
    "| A1234567A      | Jack O'Lantern       |\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Please upload to the respective LumiNUS folders for your group by Friday 11 September 2359 hours for the Tuesday 2 pm lab group, and Monday 2359 hours for the Friday 12 pm lab group.\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to create dense neural networks using Keras.\n",
    "    2. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    3. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    \n",
    "To save time we will train each experiment only for 50 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "**HINT: YOU CAN HIT SHIFT-ENTER TO RUN EACH CELL. NOTE THAT IF A CELL IS DEPENDENT ON A PREVIOUS CELL, YOU WILL NEED TO RUN THE PREVIOUS CELL(S) FIRST **\n",
    "\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 1, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 1 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Keras provides the to_categorical function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(y = iris.target, num_classes = 3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Create the neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(learning_rate = 0.1, momentum = 1.0)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "          metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 50 epochs. We will shuffle the training data between epochs, and provide validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 16:19:34.885257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 39ms/step - loss: 1.0594 - accuracy: 0.5750 - val_loss: 1.0293 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8947 - accuracy: 0.6667 - val_loss: 0.8649 - val_accuracy: 0.5667\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7109 - accuracy: 0.7083 - val_loss: 0.5666 - val_accuracy: 0.7667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4955 - accuracy: 0.8167 - val_loss: 0.4499 - val_accuracy: 0.6667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3548 - accuracy: 0.8583 - val_loss: 0.2612 - val_accuracy: 0.9667\n",
      "Epoch 6/50\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2923 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 16:19:36.615586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2614 - accuracy: 0.9333 - val_loss: 0.2212 - val_accuracy: 0.9667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1779 - accuracy: 0.9500 - val_loss: 0.0969 - val_accuracy: 0.9667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1577 - accuracy: 0.9250 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1434 - accuracy: 0.9333 - val_loss: 0.0832 - val_accuracy: 0.9667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1015 - accuracy: 0.9500 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1681 - accuracy: 0.9167 - val_loss: 0.0889 - val_accuracy: 0.9667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9750 - val_loss: 0.0694 - val_accuracy: 0.9667\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1129 - accuracy: 0.9417 - val_loss: 0.0997 - val_accuracy: 0.9667\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0945 - accuracy: 0.9667 - val_loss: 0.1025 - val_accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0811 - accuracy: 0.9417 - val_loss: 0.1301 - val_accuracy: 0.9667\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0840 - accuracy: 0.9583 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0817 - accuracy: 0.9667 - val_loss: 0.1508 - val_accuracy: 0.9667\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1404 - accuracy: 0.9667 - val_loss: 3.6352e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2849 - accuracy: 0.9500 - val_loss: 1.1017 - val_accuracy: 0.9000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9035 - accuracy: 0.9333 - val_loss: 0.1897 - val_accuracy: 0.9667\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.6303 - accuracy: 0.8917 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.7677 - accuracy: 0.9167 - val_loss: 2.8901 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9119 - accuracy: 0.9500 - val_loss: 0.6104 - val_accuracy: 0.9333\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.7012 - accuracy: 0.9083 - val_loss: 0.5643 - val_accuracy: 0.9667\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5990 - accuracy: 0.9667 - val_loss: 0.5136 - val_accuracy: 0.9667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4818 - accuracy: 0.9667 - val_loss: 8.2630e-09 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3291 - accuracy: 0.9583 - val_loss: 0.2952 - val_accuracy: 0.9667\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5838 - accuracy: 0.9750 - val_loss: 0.7508 - val_accuracy: 0.9667\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5245 - accuracy: 0.9667 - val_loss: 0.1162 - val_accuracy: 0.9667\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3065 - accuracy: 0.9167 - val_loss: 0.0796 - val_accuracy: 0.9667\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2099 - accuracy: 0.9750 - val_loss: 0.7056 - val_accuracy: 0.9667\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1745 - accuracy: 0.9667 - val_loss: 1.0031 - val_accuracy: 0.9667\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6632 - accuracy: 0.9750 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2087 - accuracy: 0.9750 - val_loss: 5.5086e-09 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5727 - accuracy: 0.9583 - val_loss: 6.6829e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3556 - accuracy: 0.9667 - val_loss: 0.4590 - val_accuracy: 0.9667\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3309 - accuracy: 0.9750 - val_loss: 2.7970e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3120 - accuracy: 0.9500 - val_loss: 7.2466e-06 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4111 - accuracy: 0.9583 - val_loss: 0.4094 - val_accuracy: 0.9667\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3270 - accuracy: 0.9667 - val_loss: 0.2458 - val_accuracy: 0.9667\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0787 - accuracy: 0.9833 - val_loss: 1.5424e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1635 - accuracy: 0.9417 - val_loss: 5.2516e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9583 - val_loss: 0.1416 - val_accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1703 - accuracy: 0.9667 - val_loss: 0.1972 - val_accuracy: 0.9667\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0796 - accuracy: 0.9750 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1478 - accuracy: 0.9500 - val_loss: 2.6527e-05 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2111 - accuracy: 0.9500 - val_loss: 0.1493 - val_accuracy: 0.9667\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2577 - accuracy: 0.9667 - val_loss: 0.3401 - val_accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2681 - accuracy: 0.9583 - val_loss: 0.0360 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f572790>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, Y_train, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "**Answer: No, the consistent increase in accuracy and decrease in loss for both training and validation accuracy indicate that the network is fitting properly.  (Note to TA: If students ask why the accuracy is so low, it's because we run for only 50 epochs to save time. **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://keras.io/api/optimizers/sgd/). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**Answer: The lr parameter controls how fast the network learns **\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The documentation states that the momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum term speeds up learning while preventing the network from swinging wildly in terms of accuracy / loss, which can happen when learning rate is high **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and validation accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    | 0.6917        |       0.5667    |                   |\n",
    "|0.1     | 0.950         |       0.9667    |                   |\n",
    "|1.0     | 0.9667        |       0.9667    |                   |\n",
    "|10.0    |               |                 |                   |\n",
    "|100     |               |                 |                   |\n",
    "|1000    |               |                 |                   |\n",
    "|10000   |0.3667         |       0.2000    | Huge losses       |\n",
    "| 100000 |               |                 |                   |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**Answer: Small learning rates result in slow learning, very large learning rates result in unstable learning that can cause huge losses and low accuracy **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. Note: Do not use the Nesterov parameter - Leave it as False.\n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |   0.9500      |  0.93333        |                   |\n",
    "|0.01      |   0.9417      |  0.9333         |                   |\n",
    "|0.1       |   0.9583      |  0.9667         |                   |\n",
    "|1.0       |   0.9417      |  1.0000         |                   |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum appears to help with increasing learning rate especially for validation data **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Using Raw Unscaled Data\n",
    "\n",
    "We begin by using unscaled X and Y data. The code below will create 120 training samples and 30 testing samples (20% of the total of 150 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled = iris.data\n",
    "Y_raw = to_categorical(y = iris.target, num_classes = 3)\n",
    "X_utrain, X_utest, Y_utrain, Y_utest = train_test_split(X_unscaled, Y_raw,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "Create a new neural network called \"nn2\" below using a single hidden layer of 100 neurons. Train using the data in X_utrain, X_utest and validate with Y_utrain and Y_utest. Again use the SGD optimizer with a learning rate of 0.1 and no momentum, and train for 50 epochs. ***(3 marks)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 2.1359 - accuracy: 0.2667 - val_loss: 1.6515 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 1.3369 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 16:19:54.462508: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-08 16:19:54.626532: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9867 - accuracy: 0.5417 - val_loss: 0.6909 - val_accuracy: 0.5667\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5747 - accuracy: 0.7500 - val_loss: 0.5716 - val_accuracy: 0.6000\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5382 - accuracy: 0.6500 - val_loss: 0.4798 - val_accuracy: 0.8000\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5581 - accuracy: 0.6417 - val_loss: 0.5843 - val_accuracy: 0.5667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4334 - accuracy: 0.8500 - val_loss: 0.4246 - val_accuracy: 0.9667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4100 - accuracy: 0.8583 - val_loss: 0.3940 - val_accuracy: 0.9333\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3918 - accuracy: 0.8500 - val_loss: 0.7653 - val_accuracy: 0.5667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5103 - accuracy: 0.7083 - val_loss: 0.3762 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3790 - accuracy: 0.8167 - val_loss: 0.8432 - val_accuracy: 0.5667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5673 - accuracy: 0.7000 - val_loss: 0.3816 - val_accuracy: 0.8667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3334 - accuracy: 0.9250 - val_loss: 0.3357 - val_accuracy: 0.9333\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3238 - accuracy: 0.9333 - val_loss: 0.3595 - val_accuracy: 0.8000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5159 - accuracy: 0.6417 - val_loss: 0.3290 - val_accuracy: 0.9333\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3605 - accuracy: 0.8000 - val_loss: 0.3544 - val_accuracy: 0.8667\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2909 - accuracy: 0.9250 - val_loss: 0.4878 - val_accuracy: 0.6333\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4067 - accuracy: 0.7417 - val_loss: 0.7340 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5073 - accuracy: 0.7083 - val_loss: 0.4280 - val_accuracy: 0.7000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2926 - accuracy: 0.9083 - val_loss: 0.6886 - val_accuracy: 0.6000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3567 - accuracy: 0.8167 - val_loss: 0.8300 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5735 - accuracy: 0.6667 - val_loss: 0.5579 - val_accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2991 - accuracy: 0.8667 - val_loss: 0.2800 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2403 - accuracy: 0.9500 - val_loss: 0.2945 - val_accuracy: 0.8000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.3444 - accuracy: 0.8167 - val_loss: 0.3977 - val_accuracy: 0.8000\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5015 - accuracy: 0.7000 - val_loss: 0.2848 - val_accuracy: 0.9333\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2293 - accuracy: 0.9750 - val_loss: 0.2831 - val_accuracy: 0.8667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2146 - accuracy: 0.9667 - val_loss: 0.2540 - val_accuracy: 0.9667\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1994 - accuracy: 0.9750 - val_loss: 0.2962 - val_accuracy: 0.8667\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2162 - accuracy: 0.9083 - val_loss: 0.2595 - val_accuracy: 0.8667\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2603 - accuracy: 0.8833 - val_loss: 0.3541 - val_accuracy: 0.7333\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2701 - accuracy: 0.8750 - val_loss: 0.8878 - val_accuracy: 0.6000\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2496 - accuracy: 0.9417 - val_loss: 0.2120 - val_accuracy: 0.9333\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2017 - accuracy: 0.9083 - val_loss: 0.4029 - val_accuracy: 0.7333\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.2391 - accuracy: 0.9083 - val_loss: 0.2856 - val_accuracy: 0.8667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1885 - accuracy: 0.9667 - val_loss: 0.3573 - val_accuracy: 0.8000\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3819 - accuracy: 0.7833 - val_loss: 0.3413 - val_accuracy: 0.8000\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2858 - accuracy: 0.8750 - val_loss: 0.2701 - val_accuracy: 0.8667\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3232 - accuracy: 0.8667 - val_loss: 0.2208 - val_accuracy: 0.9667\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1598 - accuracy: 0.9750 - val_loss: 0.1912 - val_accuracy: 0.9333\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1637 - accuracy: 0.9417 - val_loss: 0.1825 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1588 - accuracy: 0.9667 - val_loss: 0.1928 - val_accuracy: 0.9333\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9583 - val_loss: 0.2062 - val_accuracy: 0.9333\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2220 - accuracy: 0.9083 - val_loss: 1.0030 - val_accuracy: 0.6000\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3734 - accuracy: 0.8500 - val_loss: 0.1726 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1856 - accuracy: 0.9083 - val_loss: 0.3281 - val_accuracy: 0.8000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.3968 - accuracy: 0.8417 - val_loss: 0.2289 - val_accuracy: 0.8667\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1471 - accuracy: 0.9667 - val_loss: 0.1678 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9667 - val_loss: 0.1907 - val_accuracy: 0.9667\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.1357 - accuracy: 0.9667 - val_loss: 0.1746 - val_accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2902 - accuracy: 0.8833 - val_loss: 0.2920 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291257e50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enter your code for Question 5 below\n",
    "\"\"\"\n",
    "\n",
    "# Create the neural network\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn2.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(learning_rate = 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn2.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "          metrics = 'accuracy')\n",
    "\n",
    "nn2.fit(X_utrain, Y_utrain, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_utest, Y_utest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Question 5 continues)**\n",
    "\n",
    "Observe the training and validation error. Does not scaling the input affect the training? Why do you think this is so? What is the advantage of scaling? ***(5 MARKS)***\n",
    "\n",
    "**Answer: Not scaling the input produces similar recognition results. However scaling can prevent overflow.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 8_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "***FOR TA ONLY***\n",
    "\n",
    "| Question |  Marks  |\n",
    "|:--------:|:-------:|\n",
    "|1         |     /4  |\n",
    "|2         |     /3  |\n",
    "|3         |     /5  |\n",
    "|4         |     /5  |\n",
    "|5         |     /8  |\n",
    "|Total:    |     /25 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
